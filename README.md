# DL Term Project on BERT (Tensorflow)
The recent advents in Recurrent Neural Networks on Natural Language Processing have facilitated the necessity towards a generalized pretrained model for arbitrary tasks on textual data. In this report, we take a closer look at some of the recent developments while proposing new methods to construct such a unified model. We propose two novel unsupervised tasks along with a graph-based word embedding. Our experimental results show the efficacy of these modalities while comparing them with the state-of-the-art solution for classification and question/answering tasks. 

The final report is available in [FinalReportBERTModification.pdf](https://github.com/mominbuet/ModifiedBERTTraining/FinalReportBERTModification.pdf)
